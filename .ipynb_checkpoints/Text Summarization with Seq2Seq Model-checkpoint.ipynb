{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/ria.json.gz\n",
      "input/ria_1k.json\n",
      "input/ria_1k_.json\n",
      "input/ria_20.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/ria_1k.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;автор мария балябина &lt;br /&gt;&lt;/strong...</td>\n",
       "      <td>украинская люстрация: жертвоприношение во власти</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;strong&gt;москва, 2...</td>\n",
       "      <td>цена на нефть марки brent поднялась выше $73 з...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;strong&gt;бангкок, ...</td>\n",
       "      <td>число пострадавших в аварии в таиланде россиян...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;strong&gt;москва, 2...</td>\n",
       "      <td>собянин открыл детсад в хорошево-мневниках в м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;&lt;strong&gt;москва, 2...</td>\n",
       "      <td>\"ташир\" построит в москве перинатально-кардиол...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  <p><strong>автор мария балябина <br /></strong...   \n",
       "1  <p><strong></strong></p>\\n<p><strong>москва, 2...   \n",
       "2  <p><strong></strong></p>\\n<p><strong>бангкок, ...   \n",
       "3  <p><strong></strong></p>\\n<p><strong>москва, 2...   \n",
       "4  <p><strong></strong></p>\\n<p><strong>москва, 2...   \n",
       "\n",
       "                                               title  \n",
       "0   украинская люстрация: жертвоприношение во власти  \n",
       "1  цена на нефть марки brent поднялась выше $73 з...  \n",
       "2  число пострадавших в аварии в таиланде россиян...  \n",
       "3  собянин открыл детсад в хорошево-мневниках в м...  \n",
       "4  \"ташир\" построит в москве перинатально-кардиол...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = dirname+filenames[1]\n",
    "print(f'{data_path}')\n",
    "\n",
    "data = pd.read_json(data_path, lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"\", text) # remove nombers\n",
    "    text = re.sub(r'\\(.*\\)','',text)\n",
    "    text = re.sub(r'[^а-яА-Я0-9. ]','',text) # remove punctuations\n",
    "    text=re.sub(\"(\\s+.\\s+)\", ' ', text) #remove any single charecters hanging between 2 spaces\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RawText=[<p><strong>автор мария балябина <br /></strong></p>\n",
      "<p style=\"float: right\"><img src=\"/images/103612/95/1036129592.jpg\" border=\"0\" alt=\"украинская люстрация\\: жертвоприношение во власти\" hspace=\"5\" width=\"1920\" height=\"1080\" tag=\"[-json: {'type':'media','injectiontype':'media','media_id':'1036129301','media_variant_type':'inject','media_title':'украинская люстрация\\\\: жертвоприношение во власти'}-]\" /></p>\n",
      "<p>на украине уволили уже больше 500 чиновников. это &ndash; люстрация в&nbsp;деле. объявл] \n",
      " ====================================================================================================\n",
      "Text=[автор мария балябина украинская люстрация жертвоприношение во власти украинская люстрация жертвоприношение во власти на украине уволили уже больше  чиновников .  это  люстрация вделе .  объявл] \n",
      " ====================================================================================================\n",
      "Title=[украинская люстрация: жертвоприношение во власти]\n"
     ]
    }
   ],
   "source": [
    "sample_text, sample_title = (data['text'][0][:500], data['title'][0])\n",
    "print(f\"RawText=[{sample_text}]\",'\\n','='*100)\n",
    "print(f\"Text=[{preprocess(sample_text)}]\",'\\n','='*100)\n",
    "print(f\"Title=[{sample_title}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocess to 'texts' and 'titles'\n",
    "data['title'] = data['title'].apply(lambda x:preprocess(x))\n",
    "data['text'] = data['text'].apply(lambda x:preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>автор мария балябина украинская люстрация жерт...</td>\n",
       "      <td>украинская люстрация жертвоприношение во власти</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>москва  дек риа новости .  цена нанефть марки ...</td>\n",
       "      <td>цена на нефть марки  поднялась выше  за баррель</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>бангкок  дек риа новости .  число россиян пост...</td>\n",
       "      <td>число пострадавших аварии таиланде россиян уве...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  автор мария балябина украинская люстрация жерт...   \n",
       "1  москва  дек риа новости .  цена нанефть марки ...   \n",
       "2  бангкок  дек риа новости .  число россиян пост...   \n",
       "\n",
       "                                               title  \n",
       "0    украинская люстрация жертвоприношение во власти  \n",
       "1    цена на нефть марки  поднялась выше  за баррель  \n",
       "2  число пострадавших аварии таиланде россиян уве...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print cleaned texts and titles\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq LSTM Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM with Attention\n",
    "# !pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    автор мария балябина украинская люстрация жерт...\n",
       "1    москва  дек риа новости .  цена нанефть марки ...\n",
       "2    бангкок  дек риа новости .  число россиян пост...\n",
       "3    москва  дек риа новости .  строительство детск...\n",
       "4    москва  декабря  риа новости .  группа компани...\n",
       "5    москва  дек рспорт .  нападающие футбольного к...\n",
       "6    москва  дек рспорт .  перед хоккеистами сборно...\n",
       "7    москва  дек риа новости .  почти  миллиона ква...\n",
       "8    москва  дек риа новости .  российская самолето...\n",
       "9    нижний новгород  дек рспорт сергей литвак .  с...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [str(doc) for doc in data['text']]\n",
    "\n",
    "# add _START_, _END_ tokens to titles\n",
    "title = ['_START_ '+ str(doc) + ' _END_' for doc in data['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=[автор мария балябина украинская люстрация жертвоприношение во власти украинская люстрация жертвоприношение во власти на украине уволили уже больше  чиновников .  это  люстрация вделе .  объявляя онача],\n",
      "\n",
      "title=[_START_ украинская люстрация жертвоприношение во власти _END_]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f'text=[{text[i][:200]}],\\n\\ntitle=[{title[i]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length distribution\n",
    "text_count = []\n",
    "summary_count = []\n",
    "for sent in text:\n",
    "    text_count.append(len(sent.split()))\n",
    "for sent in title:\n",
    "    summary_count.append(len(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df= pd.DataFrame()\n",
    "graph_df['text']=text_count\n",
    "graph_df['title']=summary_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph_df.hist(bins = 13,figsize=(15,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.70% of texts have < 500 words\n",
      "100.00% of titles have < 15 words\n"
     ]
    }
   ],
   "source": [
    "def get_text_share(text, label, threshold):\n",
    "    cnt=0\n",
    "    for i in text:\n",
    "        if(len(i.split())<=threshold):\n",
    "            cnt=cnt+1\n",
    "    print(f'{cnt/len(text):.2%} of {label}s have < {threshold} words')\n",
    "\n",
    "get_text_share(text,'text', 500)\n",
    "get_text_share(title,'title', 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model to summarize the text between 0-15 words for Summary and 0-500 words for Text\n",
    "MAX_TEXT_LEN = 500\n",
    "MAX_TITLE_LEN = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the Titles and Texts between max len defined above\n",
    "\n",
    "cleaned_text =np.array(text)\n",
    "cleaned_title=np.array(title)\n",
    "\n",
    "short_text=[]\n",
    "short_title=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_title[i].split())<=MAX_TITLE_LEN and len(cleaned_text[i].split())<=MAX_TEXT_LEN):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_title.append(cleaned_title[i])\n",
    "        \n",
    "post_pre=pd.DataFrame({'text':short_text,'title':short_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>москва  дек риа новости .  цена нанефть марки ...</td>\n",
       "      <td>_START_ цена на нефть марки  поднялась выше  з...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>бангкок  дек риа новости .  число россиян пост...</td>\n",
       "      <td>_START_ число пострадавших аварии таиланде рос...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  москва  дек риа новости .  цена нанефть марки ...   \n",
       "1  бангкок  дек риа новости .  число россиян пост...   \n",
       "\n",
       "                                               title  \n",
       "0  _START_ цена на нефть марки  поднялась выше  з...  \n",
       "1  _START_ число пострадавших аварии таиланде рос...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_pre.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add sostok and eostok at \n",
    "post_pre['title'] = post_pre['title'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>москва  дек риа новости .  цена нанефть марки ...</td>\n",
       "      <td>sostok _START_ цена на нефть марки  поднялась ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>бангкок  дек риа новости .  число россиян пост...</td>\n",
       "      <td>sostok _START_ число пострадавших аварии таила...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  москва  дек риа новости .  цена нанефть марки ...   \n",
       "1  бангкок  дек риа новости .  число россиян пост...   \n",
       "\n",
       "                                               title  \n",
       "0  sostok _START_ цена на нефть марки  поднялась ...  \n",
       "1  sostok _START_ число пострадавших аварии таила...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_pre.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQ2SEQ MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to TRAIN and VALIDATION sets\n",
    "# !pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org sklearn\n",
    "# !pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org  tensorflow\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(post_pre['text']),np.array(post_pre['title']),test_size=0.1,random_state=0,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Lets tokenize the text to get the vocab count , you can use Spacy here also\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RARE WORD ANALYSIS FOR X i.e 'text'\n",
    "tot_cnt gives the size of vocabulary (which means every unique words in the text)\n",
    "\n",
    "cnt gives me the no. of rare words whose count falls below threshold\n",
    "\n",
    "tot_cnt - cnt gives me the top most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 81.08246030078348\n",
      "Total Coverage of rare words: 28.64888077728273\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 6303\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RARE WORD ANALYSIS FOR Y i.e 'summary'\n",
    "tot_cnt gives the size of vocabulary (which means every unique words in the text)\n",
    "\n",
    "cnt gives me the no. of rare words whose count falls below threshold\n",
    "\n",
    "tot_cnt - cnt gives me the top most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 97.0453934998657\n",
      "Total Coverage of rare words: 49.056603773584904\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in Y = 111\n"
     ]
    }
   ],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now remove \"Summary\" i.e Y (both train and val) which has only START and END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)\n",
    "\n",
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary from the w2v model = 6303\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 500, 200)     1260600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 500, 300), ( 601200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 500, 300), ( 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    22200       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 500, 300), ( 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  601200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 111)    33411       lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,961,011\n",
      "Trainable params: 3,961,011\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "import gensim\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start fitting the model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 434s 62s/step - loss: 2.4714 - val_loss: 1.0870\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 517s 74s/step - loss: 1.1048 - val_loss: 1.0184\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 596s 85s/step - loss: 1.0298 - val_loss: 0.9448\n",
      "Epoch 4/50\n",
      "6/7 [========================>.....] - ETA: 1:27 - loss: 0.9766"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let’s build the dictionary to convert the index to word for target and source vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are defining a function below which is the implementation of the inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model over the data to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://www.kaggle.com/sandeepbhogaraju/text-summarization-with-seq2seq-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
